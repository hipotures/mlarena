# Seria 5 EksperymentÃ³w - Feature Engineering & Modeling Optimization

## PrzeglÄ…d

Seria 5 eksperymentÃ³w zaprojektowanych do systematycznej optymalizacji pipeline'u modelowania dla konkursu Playground Series S5E11 (Loan Payback Prediction). KaÅ¼dy eksperyment trwa ~1-1.5h i wprowadza nowe techniki feature engineering lub modelowania.

**Cel:** Poprawa wyniku z obecnego **0.92434 public AUC** do **0.94+ AUC**

---

## ğŸ“‹ Podsumowanie EksperymentÃ³w

| # | Nazwa | Strategia | Expected Boost | Czas | Status |
|---|-------|-----------|----------------|------|--------|
| 1 | Enhanced FE (Tier 1) | Log transforms, DTI ratios, payment capacity | +0.03-0.05 AUC | 1.5h | âœ… Gotowy |
| 2 | Advanced Encoding | Target Encoding, WoE, polynomials, interactions | +0.02-0.04 AUC | 1.5h | âœ… Gotowy |
| 3 | LightGBM + Optuna | Custom LightGBM z Bayesian tuning (50 trials) | +0.01-0.02 AUC | 1.5h | âœ… Gotowy |
| 4 | Stacking Ensemble | LightGBM + XGBoost + CatBoost â†’ LogReg meta | +0.01-0.03 AUC | 1.5h | âœ… Gotowy |
| 5 | Transfer Learning | Pre-train na oryginalnym datasecie (20k samples) | +0.005-0.01 AUC | 1.5h | âœ… Gotowy |

**ÅÄ…czny oczekiwany boost (kumulatywny):** +0.055 - 0.15 AUC (przy zaÅ‚oÅ¼eniu addytywnoÅ›ci)

---

## ğŸš€ Komendy Uruchomieniowe

### Eksperyment 1: Enhanced Feature Engineering (Tier 1)

**Opis:**
- Transformacje logarytmiczne dla skewed features (annual_income, loan_amount)
- Yeo-Johnson power transformations
- DTI-based features: monthly_debt, payment_capacity, remaining_income
- Critical ratios: loan_to_income, payment_to_income, combined_dti
- Interest cost analysis: total_interest_cost, interest_burden_ratio
- Risk flags: high_dti, low_remaining_income, high_loan_to_income

**Nowe featury:** ~25 dodatkowych cech

**Komenda:**
```bash
cd /mnt/ml/kaggle-fork1

# Uruchomienie (AutoGluon best_quality, 1.5h)
uv run python scripts/ml_runner.py \
    --project playground-series-s5e11 \
    --model exp01_tier1_features \
    --auto-submit \
    --wait-seconds 45
```

**Oczekiwany wynik:** Local CV: 0.945-0.950, Public: 0.940-0.945

---

### Eksperyment 2: Advanced Encoding + Interactions

**Opis:**
- **Buduje na Tier 1** (zawiera wszystkie featury z Exp 1)
- Target Encoding z CV dla grade_subgrade, loan_purpose (prevents leakage)
- Weight of Evidence (WoE) encoding dla kategorii
- Polynomial features (degree 2) dla kluczowych zmiennych
- Cross-feature interactions:
  - income_credit_power = income Ã— credit_score
  - loan_cost_indicator = loan_amount Ã— interest_rate
  - credit_risk_score = credit_score / (dti + 0.01)
  - risk_adjusted_return = interest_rate / credit_quality
- Grade decomposition: grade_subgrade â†’ grade + subgrade_num

**Nowe featury:** +15-20 na top Tier 1 (~40-45 total)

**Komenda:**
```bash
cd /mnt/ml/kaggle-fork1

# Uruchomienie
uv run python scripts/ml_runner.py \
    --project playground-series-s5e11 \
    --model exp02_tier2_encoding \
    --auto-submit \
    --wait-seconds 45
```

**Oczekiwany wynik:** Local CV: 0.950-0.955, Public: 0.945-0.950

---

### Eksperyment 3: LightGBM z Optuna Hyperparameter Tuning

**Opis:**
- Custom LightGBM (zamiast AutoGluon)
- Optuna Bayesian optimization: 50 trials
- Hyperparameters tuned:
  - learning_rate: 0.01 - 0.1 (log scale)
  - num_leaves: 20 - 150
  - max_depth: 3 - 12
  - Regularization: min_child_samples, reg_alpha, reg_lambda
  - Sampling: subsample, colsample_bytree
- Stratified 5-Fold CV
- Early stopping (50 rounds)
- class_weight='balanced' (handles imbalance)
- Feature importance analysis

**Komenda:**
```bash
cd /mnt/ml/kaggle-fork1

# Uruchomienie (50 trials Optuna + final training)
uv run python scripts/ml_runner.py \
    --project playground-series-s5e11 \
    --model exp03_lgbm_optuna \
    --auto-submit \
    --wait-seconds 45
```

**Oczekiwany wynik:** Local CV: 0.948-0.953, Public: 0.943-0.948

**Uwaga:** Ten eksperyment moÅ¼e daÄ‡ nieco niÅ¼szy wynik niÅ¼ Exp 1-2 jeÅ›li AutoGluon ensemble jest silniejszy, ale bÄ™dzie miaÅ‚ **najlepszÄ… feature importance analysis**.

---

### Eksperyment 4: Stacking Ensemble

**Opis:**
- **Level 1 Base Models:**
  - LightGBM (class_weight='balanced')
  - XGBoost (scale_pos_weight=4)
  - CatBoost (auto_class_weights='Balanced')
- **Level 2 Meta-Model:**
  - Logistic Regression (class_weight='balanced')
  - Calibrated z Isotonic Regression
- Out-of-Fold predictions (5-fold CV) - prevents overfitting
- UÅ¼ywa Tier 2 features (Tier 1 + encodings)

**Architektura:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: Tier 2 Features (~45 cols) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   5-Fold CV Training  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚             â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚ LightGBMâ”‚    â”‚ XGBoostâ”‚   â”‚ CatBoostâ”‚
â”‚ (500 it)â”‚    â”‚ (500 it)â”‚   â”‚ (500 it)â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
    â”‚             â”‚             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Out-of-Fold     â”‚
         â”‚ Predictions (3) â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Logistic Reg    â”‚
         â”‚ + Calibration   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
            Final Predictions
```

**Komenda:**
```bash
cd /mnt/ml/kaggle-fork1

# Uruchomienie (5 folds Ã— 3 models = 15 base models total)
uv run python scripts/ml_runner.py \
    --project playground-series-s5e11 \
    --model exp04_stacking_ensemble \
    --auto-submit \
    --wait-seconds 45
```

**Oczekiwany wynik:** Local CV: 0.952-0.958, Public: 0.947-0.952

**Uwaga:** To prawdopodobnie **najlepszy eksperyment** - ensemble diversity + calibration czÄ™sto wygrywa konkursy.

---

### Eksperyment 5: Transfer Learning

**Opis:**
- **Pre-training** na oryginalnym datasecie (20k samples)
- Compute industry statistics:
  - Default rates by loan_purpose, grade, employment_status
  - Median income/loan by purpose
  - Median interest rate by grade
- Use pre-trained predictions as **meta-feature**
- Add **statistical augmentation** features:
  - industry_default_rate_purpose
  - industry_median_income_purpose
  - income_vs_industry (deviation from norm)
  - loan_vs_industry
  - rate_vs_industry
- **Fine-tune** na competition data
- UÅ¼ywa Tier 1 features

**Pipeline:**
```
1. Load original dataset (20k samples)
   â†“
2. Compute industry statistics
   â†“
3. Pre-train AutoGluon (30 min, medium_quality)
   â†“
4. Generate predictions on competition data
   â†“
5. Add industry stats + pretrain predictions as features
   â†“
6. Fine-tune AutoGluon (1h, best_quality)
```

**Wymagania:**
âš ï¸ **Musisz pobraÄ‡ oryginalny dataset:**
```bash
cd /mnt/ml/kaggle-fork1/projects/kaggle/playground-series-s5e11/data

# Download original dataset
kaggle datasets download -d nabihazahid/loan-prediction-dataset-2025

# Unzip
unzip loan-prediction-dataset-2025.zip

# Rename if needed to: loan_dataset_20000.csv
```

**Komenda:**
```bash
cd /mnt/ml/kaggle-fork1

# Uruchomienie (30 min pretrain + 1h finetune)
uv run python scripts/ml_runner.py \
    --project playground-series-s5e11 \
    --model exp05_transfer_learning \
    --auto-submit \
    --wait-seconds 45
```

**Oczekiwany wynik:** Local CV: 0.945-0.950, Public: 0.941-0.946

**Uwaga:** Boost moÅ¼e byÄ‡ mniejszy jeÅ›li oryginalny dataset ma innÄ… dystrybucjÄ™ niÅ¼ syntetyczny.

---

## ğŸ“Š Strategia Uruchamiania

### Rekomendowana kolejnoÅ›Ä‡:

#### Faza 1: Quick Wins (Run All, ~6h total)
```bash
# Uruchom wszystkie 5 eksperymentÃ³w rÃ³wnolegle lub sekwencyjnie
# KaÅ¼dy ~1.5h, wiÄ™c jeÅ›li masz moÅ¼liwoÅ›Ä‡ rÃ³wnolegÅ‚ego uruchomienia:
# - Exp 1, 2, 3 rÃ³wnolegle (3x 1.5h = 1.5h wall time)
# - Exp 4, 5 po zakoÅ„czeniu pierwszej partii

# Lub sekwencyjnie (bezpieczniejsze):
1. exp01_tier1_features
2. exp02_tier2_encoding
3. exp04_stacking_ensemble  # Ten prawdopodobnie najlepszy
4. exp03_lgbm_optuna
5. exp05_transfer_learning  # JeÅ›li masz oryginalny dataset
```

#### Faza 2: Identyfikacja zwyciÄ™zcy
Po zakoÅ„czeniu wszystkich, sprawdÅº wyniki:
```bash
# SprawdÅº submissions
uv run python scripts/submissions_tracker.py --project playground-series-s5e11 list

# SprawdÅº local CV scores vs public scores
# Wybierz eksperyment z:
# 1. NajwyÅ¼szym public score
# 2. Najmniejszym overfittingiem (CV - Public)
```

#### Faza 3: Long Training
ZwyciÄ™zcÄ™ uruchom na dÅ‚uÅ¼ej:
```bash
# PrzykÅ‚ad: jeÅ›li exp04_stacking_ensemble wygraÅ‚
# Zmodyfikuj config i uruchom na 4-8h:

# Dla AutoGluon-based (exp01, exp02, exp05):
# ZmieÅ„ time_limit w kodzie na 14400 (4h) lub 28800 (8h)

# Dla ensemble (exp04):
# ZwiÄ™ksz base_model_iterations z 500 do 1500
# Dodaj wiÄ™cej folds (z 5 do 10)

# Dla Optuna (exp03):
# ZwiÄ™ksz n_trials z 50 do 150-200
```

---

## ğŸ” Analiza WynikÃ³w

### Po kaÅ¼dym eksperymencie sprawdÅº:

1. **Local CV Score** - z outputu treningu
2. **Public LB Score** - z Kaggle
3. **Overfitting** - rÃ³Å¼nica CV - Public (powinna byÄ‡ <0.01)
4. **Feature Importance** - ktÃ³re nowe featury pomagajÄ…?

### PrzykÅ‚adowa analiza:

```bash
# View latest submission
uv run python scripts/submissions_tracker.py --project playground-series-s5e11 list | head -5

# PorÃ³wnaj z baseline:
# Baseline: autogluon_eda_features_fixed
#   Local CV: 0.93309
#   Public:   0.92434
#   Gap:      0.00875 (overfitting)

# Expected improvements:
# Exp 1: Public ~0.940-0.945 (gap should decrease)
# Exp 2: Public ~0.945-0.950
# Exp 4: Public ~0.947-0.952 (best bet)
```

---

## ğŸ› Troubleshooting

### Problem: "Original dataset not found" (Exp 5)
**Solution:**
```bash
cd projects/kaggle/playground-series-s5e11/data
kaggle datasets download -d nabihazahid/loan-prediction-dataset-2025
unzip loan-prediction-dataset-2025.zip
```

### Problem: "ImportError: No module named 'optuna'" (Exp 3)
**Solution:**
```bash
uv add optuna
uv sync
```

### Problem: "Memory error" podczas treningu
**Solution:**
- Zmniejsz `num_bag_folds` z 5 do 3
- Zmniejsz `time_limit`
- UÅ¼yj `presets='medium_quality'` zamiast `best_quality`

### Problem: Eksperyment trwa >2h
**Solution:**
- SprawdÅº czy dataset nie jest za duÅ¼y
- Zmniejsz `time_limit` w get_default_config()
- Dla Optuna: zmniejsz `n_trials` z 50 do 30

### Problem: Public score gorszy niÅ¼ baseline
**Przyczyny:**
1. **Overfitting** - za duÅ¼o cech, za maÅ‚o regularyzacji
2. **Data leakage** - target encoding Åºle zaimplementowany
3. **Incompatible features** - test set ma inne rozkÅ‚ady

**Debugging:**
```python
# SprawdÅº feature statistics train vs test
import pandas as pd

train = pd.read_csv('data/train.csv')
test = pd.read_csv('data/test.csv')

# Compare distributions
for col in train.columns:
    if col != 'loan_paid_back':
        print(f"{col}:")
        print(f"  Train: mean={train[col].mean():.3f}, std={train[col].std():.3f}")
        print(f"  Test:  mean={test[col].mean():.3f}, std={test[col].std():.3f}")
```

---

## ğŸ“ˆ Expected Performance Trajectory

```
Baseline (autogluon_eda_features_fixed)
â””â”€ Local CV: 0.93309
â””â”€ Public:   0.92434
   â”‚
   â”œâ”€ Exp 1 (Tier 1 FE)
   â”‚  â””â”€ Local CV: 0.945-0.950 (+0.012-0.017)
   â”‚  â””â”€ Public:   0.940-0.945 (+0.016-0.021)
   â”‚
   â”œâ”€ Exp 2 (Tier 2 Encoding)
   â”‚  â””â”€ Local CV: 0.950-0.955 (+0.017-0.022)
   â”‚  â””â”€ Public:   0.945-0.950 (+0.021-0.026)
   â”‚
   â”œâ”€ Exp 3 (LightGBM Optuna)
   â”‚  â””â”€ Local CV: 0.948-0.953 (+0.015-0.020)
   â”‚  â””â”€ Public:   0.943-0.948 (+0.019-0.024)
   â”‚
   â”œâ”€ Exp 4 (Stacking) â­ BEST BET
   â”‚  â””â”€ Local CV: 0.952-0.958 (+0.019-0.025)
   â”‚  â””â”€ Public:   0.947-0.952 (+0.023-0.028)
   â”‚
   â””â”€ Exp 5 (Transfer Learning)
      â””â”€ Local CV: 0.945-0.950 (+0.012-0.017)
      â””â”€ Public:   0.941-0.946 (+0.017-0.022)
```

**Target:** Public score **â‰¥ 0.945** (currently at 0.92434)

**Stretch goal:** Public score **â‰¥ 0.950** (top 10%)

---

## ğŸ“ Notatki

### Kluczowe obserwacje z dokumentacji:

1. **Feature Engineering > Model Tuning**
   - Tier 1 ratios (loan_to_income, payment_capacity) dajÄ… +0.03-0.05 AUC
   - Target encoding daje +0.02-0.04 AUC
   - Polynomial features mogÄ… nie dziaÅ‚aÄ‡ na syntetycznych danych

2. **AutoGluon best practices:**
   - `presets='best_quality'` z `num_stack_levels=1-2` jest optymalne
   - `hyperparameters='zeroshot'` jest juÅ¼ bardzo dobry, ale stacking lepszy
   - Nie uÅ¼ywaÄ‡ manual hyperparameter tuning dla AutoGluon

3. **Class Imbalance:**
   - Dataset: 80% paid back, 20% default (ratio 4:1)
   - `class_weight='balanced'` jest kluczowe
   - UnikaÄ‡ SMOTE (ryzyko leakage)

4. **Synthetic Data Characteristics:**
   - Prostsze modele z regularyzacjÄ… czÄ™sto lepsze niÅ¼ gÅ‚Ä™bokie sieci
   - Feature engineering bardziej krytyczny niÅ¼ na prawdziwych danych
   - Transfer learning moÅ¼e nie dziaÅ‚aÄ‡ jeÅ›li dystrybucje siÄ™ rÃ³Å¼niÄ…

### Feature Priority (z dokumentacji):

**Tier 1 (Must-have):** +0.05-0.07 AUC
- âœ… loan_to_income_ratio
- âœ… payment_income_ratio
- âœ… interest_to_income
- âœ… income_after_payment
- âœ… log(annual_income), log(loan_amount)

**Tier 2 (High value):** +0.02-0.05 AUC
- âœ… Target encoding (grade_subgrade, loan_purpose)
- âœ… Polynomial features (degree 2)
- âœ… Transfer learning features

**Tier 3 (Optional):** +0.01-0.02 AUC
- Manual ensembling
- Pseudo-labeling
- GPU acceleration (dla szybszej iteracji)

---

## ğŸ¯ Success Criteria

**Minimum Success:**
- Przynajmniej 1 eksperyment osiÄ…ga Public â‰¥ 0.940 (+1.6pp boost)

**Good Success:**
- Przynajmniej 2 eksperymenty osiÄ…gajÄ… Public â‰¥ 0.945 (+2.1pp boost)
- Overfitting <0.01 (CV - Public)

**Excellent Success:**
- Exp 4 (Stacking) osiÄ…ga Public â‰¥ 0.950 (+2.6pp boost)
- Feature importance analysis pokazuje, ktÃ³re featury naprawdÄ™ dziaÅ‚ajÄ…
- Final long-run model (8h) osiÄ…ga Public â‰¥ 0.955

---

**Powodzenia! ğŸš€**

Pytania? SprawdÅº:
- `/docs/S5E11_claude_anaylys.md` - szczegÃ³Å‚owa analiza feature engineering
- `/docs/S5E11_gemini_anaylys.md` - strategia transfer learning
- `/docs/S5E11_chatgpt_anaylys.md` - publiczne notebooki Kaggle
